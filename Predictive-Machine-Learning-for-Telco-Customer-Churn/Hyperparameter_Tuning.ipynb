{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Importing Related Notebooks** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import Imbalanced_Dataset\n",
    "\n",
    "estimator = Imbalanced_Dataset.estimator\n",
    "X_trainval = Imbalanced_Dataset.X_trainval\n",
    "y_trainval = Imbalanced_Dataset.y_trainval\n",
    "X_test = Imbalanced_Dataset.X_test\n",
    "y_test = Imbalanced_Dataset.y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Hyperparameter Tuning**\n",
    "\n",
    "Basically the developed algorithm model can produce more optimal performance by adjusting a set of hyperparameters from the algorithm model itself. The goal of hyperparameter tuning is to find the combination of parameters that produces the best model performance, including searching through hyperparameter tuning to find optimal values ​​based on certain evaluation metrics, such as recall score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Best Estimator Searching**\n",
    "\n",
    "The process can be executed with the technical assistance of `GridSearchCV()` using a predefined set of hyperparameter values. This algorithm will fully evaluate all possible hyperparameter combinations using cross validation. Until finally will choose a combination that is able to produce the best performance. Hyperparameter tuning is very important to do because the selected hyperparameter can have a significant impact on the learning ability of the algorithm model and is able to generalize from the data used.\n",
    "\n",
    "Based on previous findings, the algorithm model used is `GradientBoostingClassifier()` and is given resampling treatment using `RandomUnderSampler()`. For hyperparameter space used, refer to the documentation from **Scikit-learn** at [following link](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html). The main reference evaluation metrics will use recall score because it has a direct impact on predicting customers who were actually churned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;under_sampler&#x27;,\n",
       "                                        RandomUnderSampler(random_state=1995)),\n",
       "                                       (&#x27;model&#x27;,\n",
       "                                        GradientBoostingClassifier(random_state=1995))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;model__learning_rate&#x27;: [0.001, 0.1, 1],\n",
       "                         &#x27;model__loss&#x27;: [&#x27;log_loss&#x27;, &#x27;exponential&#x27;],\n",
       "                         &#x27;model__max_depth&#x27;: [3, 5, 7],\n",
       "                         &#x27;model__max_features&#x27;: [&#x27;sqrt&#x27;, &#x27;log2&#x27;, None],\n",
       "                         &#x27;model__n_estimators&#x27;: [50, 100, 150],\n",
       "                         &#x27;model__n_iter_no_change&#x27;: [5, 10, 20],\n",
       "                         &#x27;model__validation_fraction&#x27;: [0.1, 0.2, 0.3],\n",
       "                         &#x27;model__warm_start&#x27;: [True, False]},\n",
       "             scoring=&#x27;recall&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;under_sampler&#x27;,\n",
       "                                        RandomUnderSampler(random_state=1995)),\n",
       "                                       (&#x27;model&#x27;,\n",
       "                                        GradientBoostingClassifier(random_state=1995))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;model__learning_rate&#x27;: [0.001, 0.1, 1],\n",
       "                         &#x27;model__loss&#x27;: [&#x27;log_loss&#x27;, &#x27;exponential&#x27;],\n",
       "                         &#x27;model__max_depth&#x27;: [3, 5, 7],\n",
       "                         &#x27;model__max_features&#x27;: [&#x27;sqrt&#x27;, &#x27;log2&#x27;, None],\n",
       "                         &#x27;model__n_estimators&#x27;: [50, 100, 150],\n",
       "                         &#x27;model__n_iter_no_change&#x27;: [5, 10, 20],\n",
       "                         &#x27;model__validation_fraction&#x27;: [0.1, 0.2, 0.3],\n",
       "                         &#x27;model__warm_start&#x27;: [True, False]},\n",
       "             scoring=&#x27;recall&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;under_sampler&#x27;, RandomUnderSampler(random_state=1995)),\n",
       "                (&#x27;model&#x27;, GradientBoostingClassifier(random_state=1995))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomUnderSampler</label><div class=\"sk-toggleable__content\"><pre>RandomUnderSampler(random_state=1995)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier(random_state=1995)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('under_sampler',\n",
       "                                        RandomUnderSampler(random_state=1995)),\n",
       "                                       ('model',\n",
       "                                        GradientBoostingClassifier(random_state=1995))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'model__learning_rate': [0.001, 0.1, 1],\n",
       "                         'model__loss': ['log_loss', 'exponential'],\n",
       "                         'model__max_depth': [3, 5, 7],\n",
       "                         'model__max_features': ['sqrt', 'log2', None],\n",
       "                         'model__n_estimators': [50, 100, 150],\n",
       "                         'model__n_iter_no_change': [5, 10, 20],\n",
       "                         'model__validation_fraction': [0.1, 0.2, 0.3],\n",
       "                         'model__warm_start': [True, False]},\n",
       "             scoring='recall')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "model_pipeline = Pipeline([\n",
    "    (\n",
    "        'under_sampler',\n",
    "        RandomUnderSampler(random_state=1995)\n",
    "    ),\n",
    "    (\n",
    "        'model',\n",
    "        estimator\n",
    "    )\n",
    "])\n",
    "\n",
    "hyperparam_space = {\n",
    "    'model__loss':['log_loss','exponential'],\n",
    "    'model__learning_rate':[0.001,0.1,1],\n",
    "    'model__n_estimators':[50,100,150],\n",
    "    'model__max_depth':[3,5,7],\n",
    "    'model__max_features':['sqrt','log2',None],\n",
    "    'model__warm_start':[True,False],\n",
    "    'model__validation_fraction':[0.1,0.2,0.3],\n",
    "    'model__n_iter_no_change':[5,10,20]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model_pipeline,\n",
    "    param_grid=hyperparam_space,\n",
    "    scoring='recall',\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(\n",
    "    X=X_trainval,\n",
    "    y=y_trainval\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After hyperparameter tuning process, the next step is to find hyperparameter combinations that are able to provide the best algorithm model performance based on metrics recall score evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model__learning_rate': 0.001,\n",
       " 'model__loss': 'log_loss',\n",
       " 'model__max_depth': 3,\n",
       " 'model__max_features': None,\n",
       " 'model__n_estimators': 50,\n",
       " 'model__n_iter_no_change': 5,\n",
       " 'model__validation_fraction': 0.2,\n",
       " 'model__warm_start': True}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "grid_results = pd.DataFrame(data=grid_search.cv_results_)\n",
    "best_result = grid_results[grid_results['rank_test_score']<=1]\n",
    "display(best_result['params'].values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameter combination above will govern the behavior of the algorithm model in learning the given training data. This behavior will of course result in different performance from default hyperparameter of the algorithm model itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Best Estimator Testing and Comparison**\n",
    "\n",
    "Then a testing process will be carried out on the test data to measure the performance of the algorithm model in predicting new data and comparing the results with the algorithm model in the default hyperparameter condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============Default Classification Report============= \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.78      0.86       723\n",
      "           1       0.54      0.85      0.66       221\n",
      "\n",
      "    accuracy                           0.80       944\n",
      "   macro avg       0.74      0.82      0.76       944\n",
      "weighted avg       0.85      0.80      0.81       944\n",
      "\n",
      "\n",
      " ======================================================= \n",
      "\n",
      "==============Tuned Classification Report============== \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.62      0.75       723\n",
      "           1       0.43      0.93      0.58       221\n",
      "\n",
      "    accuracy                           0.69       944\n",
      "   macro avg       0.70      0.77      0.67       944\n",
      "weighted avg       0.84      0.69      0.71       944\n",
      "\n",
      "\n",
      " ======================================================= \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import average_precision_score, fbeta_score, precision_score, recall_score, classification_report\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "default_model = model_pipeline\n",
    "ap_scores, f2_scores, pr_scores, re_scores = [], [], [], []\n",
    "\n",
    "for i, model in enumerate([default_model,best_model]):\n",
    "    model.fit(\n",
    "        X=X_trainval,\n",
    "        y=y_trainval\n",
    "    )\n",
    "\n",
    "    ap_scores.append(average_precision_score(\n",
    "        y_true=y_test,\n",
    "        y_score=model.predict_proba(X_test)[:,1]\n",
    "    ))\n",
    "\n",
    "    f2_scores.append(fbeta_score(\n",
    "        y_true=y_test,\n",
    "        y_pred=model.predict(X_test),\n",
    "        beta=2\n",
    "    ))\n",
    "\n",
    "    pr_scores.append(precision_score(\n",
    "        y_true=y_test,\n",
    "        y_pred=model.predict(X_test)\n",
    "    ))\n",
    "\n",
    "    re_scores.append(recall_score(\n",
    "        y_true=y_test,\n",
    "        y_pred=model.predict(X_test)\n",
    "    ))\n",
    "\n",
    "    info = 'Tuned Classification Report' if i == 1 else 'Default Classification Report'\n",
    "    report = classification_report(\n",
    "        y_true=y_test,\n",
    "        y_pred=model.predict(X_test)\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        info.center(55,'='),\n",
    "        '\\n\\n'+report+'\\n\\n',\n",
    "        ('=').center(55,'='),'\\n'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the classification report above, the algorithm model with tuned hyperparameter is able to produce slightly better performance than the algorithm model with default hyperparameter. This can be seen in recall score evaluation metrics. On other metrics evaluations, tuned hyperparameter produces relatively worse performance as an impact of precision-recall trade-off. This indicates that the algorithm model with tuned hyperparameter is able to provide better performance and comply with business problem solutions.\n",
    "\n",
    "The following table can be used as an additional reference to see the performance of the two algorithm models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AP Score</th>\n",
       "      <th>F2 Score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Default</th>\n",
       "      <td>67.75</td>\n",
       "      <td>76.42</td>\n",
       "      <td>54.34</td>\n",
       "      <td>85.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tuned</th>\n",
       "      <td>54.90</td>\n",
       "      <td>75.09</td>\n",
       "      <td>42.62</td>\n",
       "      <td>92.76</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         AP Score  F2 Score  Precision  Recall\n",
       "Default     67.75     76.42      54.34   85.07\n",
       "Tuned       54.90     75.09      42.62   92.76"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison_result = pd.DataFrame(data={\n",
    "    'AP Score':[\n",
    "        ap_scores[0],\n",
    "        ap_scores[1]\n",
    "    ],\n",
    "    'F2 Score':[\n",
    "        f2_scores[0],\n",
    "        f2_scores[1]\n",
    "    ],\n",
    "    'Precision':[\n",
    "        pr_scores[0],\n",
    "        pr_scores[1]\n",
    "    ],\n",
    "    'Recall':[\n",
    "        re_scores[0],\n",
    "        re_scores[1]\n",
    "    ]},\n",
    "    index=['Default','Tuned'])\n",
    "\n",
    "comparison_result.apply(func=lambda x: round(\n",
    "    number=x*100,\n",
    "    ndigits=2\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
